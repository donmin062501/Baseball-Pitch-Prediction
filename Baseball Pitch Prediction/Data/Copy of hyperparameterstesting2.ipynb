{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of hyperparameterstesting2.ipynb","provenance":[{"file_id":"1-4GPb-zZTkN6Xr1LJIg7P_aQci9kDKne","timestamp":1628121409599}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LpMEb_6sHHZE"},"source":["# Data Processing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjDJkrcaGzu7","executionInfo":{"status":"ok","timestamp":1628279243989,"user_tz":240,"elapsed":3263,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}},"outputId":"aebe6f7e-bd84-4a03-b679-6f7107a06c04"},"source":["pip install pybaseball"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pybaseball in /usr/local/lib/python3.7/dist-packages (2.2.1)\n","Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.19.5)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (3.2.2)\n","Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.4.1)\n","Requirement already satisfied: tqdm>=4.50.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (4.62.0)\n","Requirement already satisfied: lxml>=4.2.1 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (4.2.6)\n","Requirement already satisfied: requests>=2.18.1 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (2.23.0)\n","Requirement already satisfied: attrs>=20.3.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (21.2.0)\n","Requirement already satisfied: pyarrow>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (3.0.0)\n","Requirement already satisfied: pygithub>=1.51 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.55)\n","Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (1.1.5)\n","Requirement already satisfied: beautifulsoup4>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from pybaseball) (4.6.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.0->pybaseball) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.0.0->pybaseball) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->pybaseball) (2018.9)\n","Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pygithub>=1.51->pybaseball) (1.4.0)\n","Requirement already satisfied: pyjwt>=2.0 in /usr/local/lib/python3.7/dist-packages (from pygithub>=1.51->pybaseball) (2.1.0)\n","Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from pygithub>=1.51->pybaseball) (1.2.12)\n","Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pynacl>=1.4.0->pygithub>=1.51->pybaseball) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub>=1.51->pybaseball) (2.20)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.1->pybaseball) (2.10)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->pygithub>=1.51->pybaseball) (1.12.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cAm16rHLG2kt","executionInfo":{"status":"ok","timestamp":1628279243989,"user_tz":240,"elapsed":3,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["from pybaseball import playerid_lookup\n","from pybaseball import statcast_pitcher\n","from pybaseball import statcast\n","from pybaseball import cache\n","\n","cache.enable()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_8ZfoqJG4o3","executionInfo":{"status":"ok","timestamp":1628279243989,"user_tz":240,"elapsed":2,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHR7A5CXG9A-","executionInfo":{"status":"ok","timestamp":1628279246787,"user_tz":240,"elapsed":98,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["def get_statcast_data(pitcher_csv, batter_csv):\n","\n","  '''\n","  function for obtaining dataframes from Statcast\n","\n","  parameters_type:\n","    (pitcher_csv, batter_csv) --> pitcher_dataframe, batter_dataframe\n","\n","  important_note:\n","    these Statcast dataframes will be concatenated to pybaseball dataframes at the end.\n","  '''\n","\n","  # read pitcher/batter csv file\n","  df_pitchers = pd.read_csv(pitcher_csv) \n","  df_batters = pd.read_csv(batter_csv)\n","\n","  # sort a df by 'last name' and 'run_value_per_100'\n","  # drop unnecessary columns\n","  df_pitchers =  df_pitchers.sort_values(by = ['last_name', 'run_value_per_100'], ascending = (True, False)).drop(['pitch_type', 'run_value', 'team_name_alt', \n","                                                                                                                   'est_ba', 'est_slg', 'est_woba',\n","                                                                                                                   'hard_hit_percent', 'pitches',\n","                                                                                                                   'pitch_usage', 'pa', 'put_away',\n","                                                                                                                   ' first_name', 'last_name'], axis =1)\n","  \n","  df_batters =  df_batters.sort_values(by = ['last_name', 'run_value_per_100'], ascending = (True, False)).drop(['pitch_type', 'run_value', 'team_name_alt',\n","                                                                                                                 'est_ba', 'est_slg', 'est_woba',\n","                                                                                                                 'hard_hit_percent', 'pitches',\n","                                                                                                                 'pitch_usage', 'pa', 'put_away',\n","                                                                                                                 ' first_name', 'last_name'], axis =1)\n","\n","  # rename some columns in order to merge with pybaseball dataframe (for future purposes)                                               \n","  df_pitchers.rename(columns={'run_value_per_100': 'p_run_value_per_100',\n","                              'ba':'p_ba',\n","                              'slg':'p_slg', \n","                              'woba': 'p_woba', \n","                              'whiff_percent': 'p_whiff_percent', \n","                              'k_percent': 'p_k_percent',\n","                              'player_id': 'pitcher_id',\n","                              'pitch_name':'pitch_type'}, inplace=True)\n","  \n","  df_batters.rename(columns={'run_value_per_100': 'b_run_value_per_100', \n","                             'ba':'b_ba', \n","                             'slg':'b_slg', \n","                             'woba': 'b_woba', \n","                             'whiff_percent': 'b_whiff_percent', \n","                             'k_percent': 'b_k_percent',\n","                             'player_id': 'batter_id',\n","                             'pitch_name':'pitch_type'}, inplace=True)\n","                                               \n","  return df_pitchers, df_batters"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2-BjYSzHDTV","executionInfo":{"status":"ok","timestamp":1628279248636,"user_tz":240,"elapsed":257,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["def get_pybaseball_data(start_date, end_date, pitcher_data, batter_data):\n","\n","    '''\n","    function for obtaining pybaseball dataframes and finalizing into a final dataframe\n","\n","    parameters_type:\n","    (start_date, end_date, pitcher_dataframe, batter_dataframe) --> final dataframe\n","\n","    important_note:\n","    pitcher_dataframe & batter_dataframe are the dataframes obtained using get_statcast_data function!\n","    '''\n","\n","\n","    # API call using pybaseball library\n","    data = statcast(start_dt=start_date, end_dt=end_date)\n","\n","\n","    # drop unnecessary columns\n","    data = data.drop(['release_speed', 'release_pos_x', 'release_pos_z', 'spin_dir',\n","                    'spin_rate_deprecated', 'break_angle_deprecated', 'break_length_deprecated',\n","                    'zone', 'game_type', 'hit_location', 'bb_type', 'pfx_x', 'pfx_z',\n","                    'plate_x', 'plate_z','hc_x', 'hc_y', 'tfs_deprecated', 'tfs_zulu_deprecated',\n","                    'fielder_2', 'umpire', 'sv_id', 'vx0', 'vy0', 'vz0', 'ax', 'ay', 'az', 'sz_top',\n","                    'sz_bot', 'hit_distance_sc', 'launch_speed', 'launch_angle', 'effective_speed',\n","                    'release_spin_rate', 'release_extension', 'pitcher.1', 'fielder_2.1', 'fielder_3',\n","                    'fielder_4', 'fielder_5', 'fielder_6', 'fielder_7', 'fielder_8', 'fielder_9', \n","                    'release_pos_y', 'estimated_ba_using_speedangle', 'estimated_woba_using_speedangle',\n","                    'woba_value', 'woba_denom', 'babip_value', 'iso_value', 'launch_speed_angle',\n","                    'bat_score', 'fld_score', 'post_bat_score', 'post_fld_score', \n","                    'if_fielding_alignment', 'of_fielding_alignment', 'spin_axis', 'delta_home_win_exp', \n","                    'delta_run_exp', 'pitch_type'], axis = 1)\n","\n","    # rename to follow the same convention as Statcast dataframes\n","    data.rename(columns={'player_name': 'pitcher_name'}, inplace=True)\n","\n","\n","    # combine ball_count and save as one column called 'count'\n","    # follow the basic convention --> 'ball'-'strike'\n","    # ie) '1-2' : 1 ball, 2 strikes\n","\n","    count = []\n","\n","    for ball, strike in zip(data['balls'].tolist(), data['strikes'].tolist()):\n","        combined = str(ball) + '-' + str(strike)\n","        count.append(combined)\n","\n","    data['count'] = count\n","    data = data.drop(['balls', 'strikes'], axis=1)\n","\n","\n","    # reorganize runners on bases columns\n","    # 1 --> runner is on the base\n","    # 0 --> runner is not on the base\n","    # ie) if runners are on the first and third base only,\n","    #       'on_3b' : 1\n","    #       'on_2b' : 0\n","    #       'on_1b' : 1\n","\n","    values = {'on_3b': 0, 'on_2b': 0, 'on_1b': 0}\n","    data = data.fillna(value=values)\n","\n","    on_third_base = []\n","    on_second_base = []\n","    on_first_base = []\n","\n","    for i in data['on_3b'].tolist():\n","        if i != 0:\n","            on_third_base.append(1)\n","        else:\n","            on_third_base.append(0)\n","\n","    for j in data['on_2b'].tolist(): \n","        if j != 0:\n","            on_second_base.append(1)\n","        else:\n","            on_second_base.append(0)\n","\n","    for k in data['on_1b'].tolist():\n","        if k != 0:\n","            on_first_base.append(1)\n","        else:\n","            on_first_base.append(0)\n","\n","    data['on_3b'] = on_third_base\n","\n","    data['on_2b'] = on_second_base\n","\n","    data['on_1b'] = on_first_base\n","\n","\n","    # exception handling: removing the row with infeasible ball_count (ie. 4-1)\n","    infeasible_indices = data[data['count']=='4-1'].index\n","\n","    if len(infeasible_indices) > 0:\n","        data = data.drop(infeasible_indices, axis=0)\n","\n","\n","    # extract batter names from 'des' column & save under a column called 'batter_name'\n","    # key_note:\n","    #   first two words are the batter names\n","\n","    str_des = []\n","    for des in data['des'].tolist():\n","        str_des.append(str(des))\n","\n","    data['des'] = str_des\n","\n","    data['batter_name'] = data['des'].apply(lambda x: ', '.join(x.split()[:2][::-1]))\n","    data= data.drop('des', axis =1) # drop 'des' column since it's not needed anymore\n","\n","\n","    # reorder the columns\n","    data = data[[\n","    'game_pk',\n","    'game_date',\n","    'game_year',\n","    'home_team',\n","    'away_team',\n","    'pitcher_name',\n","    'p_throws',\n","    'pitcher',\n","    'pitch_number',\n","    'batter_name',\n","    'stand',\n","    'batter',\n","    'at_bat_number',\n","    'inning',\n","    'inning_topbot',\n","    'outs_when_up',\n","    'count',\n","    'on_3b',\n","    'on_2b',\n","    'on_1b',\n","    'type',\n","    'pitch_name',\n","    'events',\n","    'description',\n","    'home_score',\n","    'away_score',\n","    'post_home_score',\n","    'post_away_score'\n","    ]]\n","\n","\n","    # rename few columns in the same convention that is used in Statcast dataframe\n","    data.rename(columns={'pitcher': 'pitcher_id',\n","                       'batter':'batter_id', \n","                       'game_pk': 'game_id', \n","                       'pitch_name':'pitch_type',\n","                       'stand':'b_bats'}, inplace=True)\n","\n","\n","    # extra EDA process, in which similar pitch_types are clustered into one pitch_type\n","    data.replace({'Knuckle Curve':'Curveball',\n","                'Split-Finger': 'Splitter',\n","                '2-Seam Fastball': 'Sinker',\n","                '4-Seam Fastball' : '4-Seamer'}, inplace=True)\n","\n","    # drop some uncommon pitch_types (only ~1% of the entire dataset)\n","    data = data.drop(data[data['pitch_type'] == 'Knuckleball'].index)\n","    data = data.drop(data[data['pitch_type'] == 'Eephus'].index)\n","    data = data.drop(data[data['pitch_type'] == 'Fastball'].index)\n","\n","    # drop additional missing rows\n","    data['events'].fillna('None', inplace=True)\n","    data.dropna(axis='rows', inplace=True)\n","\n","    # merge pybaseball dataframe with Statcast dataframe\n","    data = data.merge(pitcher_data, on=['pitcher_id','pitch_type'])\n","    data = data.merge(batter_data, on=['batter_id','pitch_type'])\n","    data.drop(columns=['events', 'description'], axis=1, inplace=True)\n","\n","    \n","    # to make data into a correct chronological, play-by-play format, we sort the columns accordingly\n","    data = data.sort_values(by = ['game_date', 'game_id', 'inning', 'inning_topbot', 'at_bat_number', 'pitch_number'], \n","                         ascending = (True, True, True, False, True, True))\n","\n","\n","\n","    return data"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJsWWB_XTzj4"},"source":["# Further Data Processing "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"RU6kRMW0T4re","executionInfo":{"status":"error","timestamp":1628279254236,"user_tz":240,"elapsed":104,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}},"outputId":"0e6e4dcb-c8df-4774-8bdf-92068c74d01c"},"source":["###### 2019 DATA ######\n","pitcher_data, batter_data = get_statcast_data('2019/Pitch-Arsenal-Pitchers-2019.csv', '2019/Pitch-Arsenal-Batters-2019.csv')\n","data_2019 = get_pybaseball_data('2019-03-20', '2019-09-29' , pitcher_data, batter_data)\n","\n","###### 2020 DATA ######\n","pitcher_data, batter_data = get_statcast_data('2020/Pitch-Arsenal-Pitchers-2020.csv', '2020/Pitch-Arsenal-Batters-2020.csv')\n","data_2020 = get_pybaseball_data('2020-07-23', '2020-09-27' , pitcher_data, batter_data)\n","\n","###### 2021 DATA ######\n","pitcher_data, batter_data = get_statcast_data('2021/Pitch-Arsenal-Pitchers-2021.csv', '2021/Pitch-Arsenal-Batters-2021.csv')\n","data_2021 = get_pybaseball_data('2021-04-01', '2021-06-15', pitcher_data, batter_data)\n","\n","###### CONCATENATE 2019, 2020, 2021 DATA ######\n","frames = [data_2019, data_2020, data_2021]\n","\n","final_df = pd.concat(frames)\n","final_df = final_df.groupby('game_id').filter(lambda x : len(x) >= 200) # Kept rows where there were more than 200 pitches\n","# Less than 200 pitches accounted for less than 2 % of the data\n"],"execution_count":18,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-8fe9de8a1ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m###### 2019 DATA ######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpitcher_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatter_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_statcast_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2019/Pitch-Arsenal-Pitchers-2019.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2019/Pitch-Arsenal-Batters-2019.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_2019\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pybaseball_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2019-03-20'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2019-09-29'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpitcher_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatter_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m###### 2020 DATA ######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-53d5abc352a4>\u001b[0m in \u001b[0;36mget_statcast_data\u001b[0;34m(pitcher_csv, batter_csv)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# read pitcher/batter csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mdf_pitchers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpitcher_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mdf_batters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatter_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2019/Pitch-Arsenal-Pitchers-2019.csv'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"fS2DyaZ3AFgX","executionInfo":{"status":"error","timestamp":1628279269930,"user_tz":240,"elapsed":106,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}},"outputId":"ad976571-8776-4208-c035-85341950d4dd"},"source":["final_df.columns\n","data_2022 = statcast('2019-04-01', '2019-04-01')\n","data_2022['pitch_number']"],"execution_count":19,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-7e3dde0ebe9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_2022\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2019-04-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2019-04-01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_2022\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pitch_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"]}]},{"cell_type":"code","metadata":{"id":"8U85g_MyAFgY","executionInfo":{"status":"aborted","timestamp":1628279213698,"user_tz":240,"elapsed":248,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["data_2022['pitch_number'].unique()\n","\n","# pitch_number == at_bat_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWywV-R2AFgY","executionInfo":{"status":"aborted","timestamp":1628279213698,"user_tz":240,"elapsed":248,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["final_df[final_df['pitcher_name'] == 'Scherzer, Max']['pitcher_id']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gaoq1I6T5nh","executionInfo":{"status":"aborted","timestamp":1628279213698,"user_tz":240,"elapsed":248,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["###### FIND THE PROPER ORDER FOR EACH GAME_ID'S COUNTS (FOR FUTURE SPLITTING PURPOSES) ######\n","\n","dfd = pd.DataFrame(final_df['game_id'].value_counts())\n","dfd['count'] = dfd.index.values\n","dfd.rename(columns = {'game_id': 'count', 'count':'game_id'}, inplace = True)\n","dfd.head()\n","\n","dfd_dict = {}\n","\n","for i, j in zip(dfd['count'].tolist(), dfd['game_id'].tolist()):\n","    dfd_dict[j] = i\n","\n","\n","actual_id_count = {}\n","\n","for g_id in np.array(final_df['game_id'].unique()).tolist():\n","    if g_id in dfd_dict:\n","        actual_id_count[g_id] = dfd_dict[g_id]\n","    else:\n","        pass\n","\n","###### PLAYER ID MAPPING ######\n","\n","player_dict = {}\n","\n","for pitcher_name, batter_name, p_id, b_id in zip(final_df['pitcher_name'].unique().tolist(), final_df['batter_name'].unique().tolist(),\n","                                                np.array(final_df['pitcher_id'].unique()).tolist(), np.array(final_df['batter_id'].unique()).tolist()):\n","    player_dict[p_id] = pitcher_name\n","    player_dict[b_id] = batter_name\n","    \n","print(list(player_dict.keys())[list(player_dict.values()).index('Scherzer, Max')])\n","# 501571"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9v3TldFmT52g","executionInfo":{"status":"aborted","timestamp":1628279213699,"user_tz":240,"elapsed":249,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["###### SHUFFLING DATA ######\n","\n","\n","lst_value_cnt = list(actual_id_count.values())     \n","\n","lst_value_cnt_sum= []\n","for i in range(len(lst_value_cnt)):\n","    lst_value_cnt_sum.append(sum(lst_value_cnt[:i+1]))\n","\n","\n","lst = final_df.values.tolist()\n","\n","val_check = 0\n","idx = 0\n","tmp_idx = 0\n","\n","final_lst = []\n","for val in lst:\n","    if val_check != val[0]:\n","        temp = lst[tmp_idx:lst_value_cnt_sum[idx]]\n","        tmp_idx = lst_value_cnt_sum[idx]\n","        idx += 1\n","        final_lst.append(temp)\n","        val_check = val[0]\n","\n","    else:\n","        pass\n","\n","import random \n","random.seed(101)\n","random.shuffle(final_lst)\n","shuffled_lst = final_lst\n","\n","\n","###### CREATE 'MINI-DATAFRAMES' BY ITERATING THROUGH THE LIST, AND THEN MERGE INTO FINAL DATAFRAME ######\n","\n","lst_of_randomized_dfs = []\n","\n","for mini_df in shuffled_lst: \n","    df = pd.DataFrame(mini_df,columns=final_df.columns.tolist())\n","    lst_of_randomized_dfs.append(df) \n","\n","shuffled_df = pd.concat(lst_of_randomized_dfs)\n","shuffled_df\n","\n","\n","###### DROP SOME COLUMNS FOR THE LAST TIME (SOME COLUMNS EXISTED PREVIOUSLY PURELY FOR SORTING PURPOSES))######\n","shuffled_df.columns\n","shuffled_df = shuffled_df.drop(['pitch_number','at_bat_number', 'pitcher_name', 'batter_name', 'game_year'], axis = 1)\n","\n","# DROP PITCH_NUMBER LATER\n","\n","###### REORDER DATAFRAME INTO STATIONARY + SEQUENTIAL COLUMNS  (BASIC IDEA)--> [[STATIONARY COLUMNS], [SEQUENTIAL COLUMNS], [LABEL]] ######\n","\n","shuffled_df = shuffled_df[['pitcher_id', 'batter_id', 'p_run_value_per_100', 'p_ba',\n","       'p_slg', 'p_woba', 'p_whiff_percent', 'p_k_percent',\n","       'b_run_value_per_100', 'b_ba', 'b_slg', 'b_woba', 'b_whiff_percent',\n","       'b_k_percent', 'game_id', 'home_team', 'away_team', 'inning', 'inning_topbot', 'outs_when_up', 'count',\n","       'on_3b', 'on_2b', 'on_1b','type', 'home_score', 'away_score',\n","       'post_home_score', 'post_away_score', 'pitch_type']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yKnMkpkAFgZ","executionInfo":{"status":"aborted","timestamp":1628279213699,"user_tz":240,"elapsed":148,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# 453286\n","shuffled_df = shuffled_df[shuffled_df['pitcher_id'] == 453286]\n","shuffled_df = shuffled_df[['batter_id', 'p_run_value_per_100', 'p_ba',\n","       'p_slg', 'p_woba', 'p_whiff_percent', 'p_k_percent',\n","       'b_run_value_per_100', 'b_ba', 'b_slg', 'b_woba', 'b_whiff_percent',\n","       'b_k_percent', 'inning', 'inning_topbot', 'outs_when_up', 'count',\n","       'on_3b', 'on_2b', 'on_1b','type', 'pitch_type']]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwF7rxo9AFga","executionInfo":{"status":"aborted","timestamp":1628279213699,"user_tz":240,"elapsed":148,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["### TEMP ####\n","\n","list_of_rows = []\n","df_list = []\n","        \n","for i in range(len(shuffled_df) - 1):\n","    curr_id = shuffled_df.iloc[[i]].values[0,0]\n","    next_id = shuffled_df.iloc[[i + 1]].values[0,0]\n","    \n","    if curr_id == next_id:\n","        temp_df = shuffled_df.iloc[[i]].values[0] # add loc here for columns\n","        list_of_rows.append(temp_df)\n","    \n","    if curr_id != next_id:\n","        temp_df = shuffled_df.iloc[[i]].values[0]\n","        list_of_rows.append(temp_df)\n","        df_list.append(pd.DataFrame(list_of_rows, columns = shuffled_df.columns))\n","        list_of_rows.clear()\n","    \n","    else:\n","        pass\n","        \n","\n","df_list[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCNfAvB8AFga","executionInfo":{"status":"aborted","timestamp":1628279213700,"user_tz":240,"elapsed":149,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# shuffled_df[['pitch_number', 'batter_id']].value_counts().tolist()[0]\n","shuffled_df.head(20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GK5brPnz99rE"},"source":["# Data Splitting"]},{"cell_type":"code","metadata":{"id":"WdIMbo3KVROg","executionInfo":{"status":"aborted","timestamp":1628279213700,"user_tz":240,"elapsed":149,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["###### ONE-HOT ENCODING ######\n","\n","shuffled_df = pd.get_dummies(shuffled_df)\n","\n","### LEAVE THIS LINE DO NOT DELETEEEE\n","# shuffled_df.columns\n","\n","# def normalize(df):\n","#     result = df.copy()\n","#     for feature_name in df.columns:\n","#         max_value = df[feature_name].max()\n","#         min_value = df[feature_name].min()\n","#         result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n","#     return result\n","\n","\n","# shuffled_df = normalize(shuffled_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AI0ei_pqAFga","executionInfo":{"status":"aborted","timestamp":1628279213700,"user_tz":240,"elapsed":148,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["list_of_rows = []\n","df_list = []\n","        \n","for i in range(len(shuffled_df) - 1):\n","    curr_id = shuffled_df.iloc[[i]].values[0,0]\n","    next_id = shuffled_df.iloc[[i + 1]].values[0,0]\n","    \n","    if curr_id == next_id:\n","        temp_df = shuffled_df.iloc[[i]].values[0] # add loc here for columns\n","        list_of_rows.append(temp_df)\n","    \n","    if curr_id != next_id:\n","        temp_df = shuffled_df.iloc[[i]].values[0]\n","        list_of_rows.append(temp_df)\n","        df_list.append(pd.DataFrame(list_of_rows, columns = shuffled_df.columns))\n","        list_of_rows.clear()\n","    \n","    else:\n","        pass\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPPf6gY1AFga","executionInfo":{"status":"aborted","timestamp":1628279213700,"user_tz":240,"elapsed":148,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# df_list[0][:4].loc[:, 'batter_id': 'b_k_percent']\n","df_list[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCk4oKfDAFgb","executionInfo":{"status":"aborted","timestamp":1628279213701,"user_tz":240,"elapsed":149,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["df_final = []\n","\n","for df in df_list:\n","    df_length = df.shape[0]\n","    if df_length <= 1:\n","        continue\n","    for i in range(2,df_length+1):\n","        df_temp = df[:i]\n","        label = list(df_temp.iloc[i-1, 35:40])\n","        stationary = list(df_temp.iloc[i-1, 0:13])\n","        df_temp.drop(df_temp.columns[0:13], axis=1, inplace=True)\n","        df_temp.drop(df_temp.columns[35:40], axis=1, inplace=True)\n","        df_temp.reset_index(drop=True, inplace=True)\n","        df_final += [(df_temp[:i-1], stationary, label)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXHugWhOAFgb","executionInfo":{"status":"aborted","timestamp":1628279213701,"user_tz":240,"elapsed":149,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# df_list[0][:2].iloc[2-1, 13:35]\n","# temp = df_list[0][:2]\n","# # temp = temp.drop(temp.columns[0:13], axis = 1)\n","# # temp = temp.drop(temp.columns[35:40], axis = 1)\n","# # temp = temp.reset_index(drop=True)\n","# temp.iloc[2-1, 0:13]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nd3RQQFwAFgb","executionInfo":{"status":"aborted","timestamp":1628279213701,"user_tz":240,"elapsed":149,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["max_length = 0 \n","for i in range(len(df_final)):\n","    if max_length < len(df_final[i][0]):\n","        max_length = len(df_final[i][0])\n","    else:\n","        pass\n","print(max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2j4Kexs9AFgb","executionInfo":{"status":"aborted","timestamp":1628279213701,"user_tz":240,"elapsed":149,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["pitch_tensors = []\n","\n","for data in df_final:\n","    tens = data[0].values\n","    pitch_tensors += [(tens.astype(float), data[1], data[2])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbcq-2Z5AFgb","executionInfo":{"status":"aborted","timestamp":1628279213702,"user_tz":240,"elapsed":150,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["df_final_by_type = [[] for _ in range(len(pitch_tensors[0][2]))]\n","\n","for row in pitch_tensors:\n","    max_idx = np.asarray(row[2]).argmax()\n","    df_final_by_type[max_idx] += [row]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"prH3IcYHAFgc","executionInfo":{"status":"aborted","timestamp":1628279213702,"user_tz":240,"elapsed":150,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["for row in df_final_by_type:\n","    print(len(row))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJSiXfFGAFgc","executionInfo":{"status":"aborted","timestamp":1628279213702,"user_tz":240,"elapsed":150,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# DO NOT RUN THIS MORE THAN ONCE\n","\n","df_final_by_type[0] *= 2\n","df_final_by_type[1] *= 5\n","df_final_by_type[2] *= 14\n","df_final_by_type[3] *= 10\n","df_final_by_type[4] *= 5\n","\n","\n","df_final_types_combined = []\n","\n","for row in df_final_by_type:\n","    df_final_types_combined.extend(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tA3mIgMAAFgc","executionInfo":{"status":"aborted","timestamp":1628279213703,"user_tz":240,"elapsed":151,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["for row in df_final_by_type:\n","    print(len(row))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"razOHjUjAFgc","executionInfo":{"status":"aborted","timestamp":1628279213703,"user_tz":240,"elapsed":151,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["for row in df_final_types_combined:\n","    print('======================================')\n","    print(row[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IaHpT39dAFgc","executionInfo":{"status":"aborted","timestamp":1628279213703,"user_tz":240,"elapsed":151,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["tensors_final = [[] for _ in range(14)]\n","\n","for row in df_final_types_combined:\n","    idx = row[0].shape[0] - 1\n","    tensors_final[idx] += [row]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ewsL_xm6AFgc","executionInfo":{"status":"aborted","timestamp":1628279213704,"user_tz":240,"elapsed":152,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["for tensors in tensors_final:\n","    print('===================')\n","    print(len(tensors))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eX6ewiZQAFgc","executionInfo":{"status":"aborted","timestamp":1628279213704,"user_tz":240,"elapsed":152,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# DO NOT RUN THIS MORE THAN ONCE\n","\n","tensors_final[2] *= 2\n","tensors_final[3] *= 3\n","tensors_final[4] *= 6\n","tensors_final[5] *= 15\n","tensors_final[6] *= 57\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HS0Ra0fHAFgc","executionInfo":{"status":"aborted","timestamp":1628279213706,"user_tz":240,"elapsed":154,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["for tensors in tensors_final:\n","    print('===================')\n","    print(len(tensors))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ky4IL4PiAFgd","executionInfo":{"status":"aborted","timestamp":1628279213706,"user_tz":240,"elapsed":12,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class PitchDataset(Dataset):\n","    def __init__(self, data_list):\n","        self.data_list = data_list\n","\n","    def __len__(self):\n","        return len(self.data_list)\n","\n","    def __getitem__(self, idx):\n","        pitch_data = self.data_list[idx][0]\n","        stationary_data = self.data_list[idx][1]\n","        label = self.data_list[idx][2]\n","        label_idx = np.asarray(label).argmax()\n","        return pitch_data, np.asarray(stationary_data, dtype=np.float64), label_idx.astype(np.float64)\n","    \n","    \n","    \n","from __future__ import print_function\n","import torchvision\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","\n","\n","class MyIter(object):\n","  \"\"\"An iterator.\"\"\"\n","  def __init__(self, my_loader):\n","    self.my_loader = my_loader\n","    self.loader_iters = [iter(loader) for loader in self.my_loader.loaders]\n","\n","  def __iter__(self):\n","    return self\n","\n","  def __next__(self):\n","    # When the shortest loader (the one with minimum number of batches)\n","    # terminates, this iterator will terminates.\n","    # The `StopIteration` raised inside that shortest loader's `__next__`\n","    # method will in turn gets out of this `__next__` method.\n","    batches = [loader_iter.next() for loader_iter in self.loader_iters]\n","    return self.my_loader.combine_batch(batches)\n","\n","  # Python 2 compatibility\n","  next = __next__\n","\n","  def __len__(self):\n","    return len(self.my_loader)\n","\n","  \n","class MyLoader(object):\n","  \"\"\"This class wraps several pytorch DataLoader objects, allowing each time \n","  taking a batch from each of them and then combining these several batches \n","  into one. This class mimics the `for batch in loader:` interface of \n","  pytorch `DataLoader`.\n","  Args: \n","    loaders: a list or tuple of pytorch DataLoader objects\n","  \"\"\"\n","  def __init__(self, loaders):\n","    self.loaders = loaders\n","\n","  def __iter__(self):\n","    return MyIter(self)\n","\n","  def __len__(self):\n","    return min([len(loader) for loader in self.loaders])\n","\n","  # Customize the behavior of combining batches here.\n","  def combine_batch(self, batches):\n","    return batches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RJDmrhYAFgd","executionInfo":{"status":"aborted","timestamp":1628279213706,"user_tz":240,"elapsed":12,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","train_datasets, valid_datasets, test_datasets = [], [], []\n","\n","# Split 70:15:15\n","\n","for data in tensors_final[0:7]:\n","    X_train, X_test, _, _ = train_test_split(data, [0 for _ in range(len(data))], test_size=0.15, random_state=1)\n","    X_train, X_val, _, _ = train_test_split(X_train, [0 for _ in range(len(X_train))], test_size=0.15, random_state=1) # 0.25 x 0.8 = 0.2\n","    train_datasets += [PitchDataset(X_train)]\n","    valid_datasets += [PitchDataset(X_val)]\n","    test_datasets += [PitchDataset(X_test)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BU-vM3SOAFgd","executionInfo":{"status":"aborted","timestamp":1628279213707,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["# For Model\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-EVS66uAFgd","executionInfo":{"status":"aborted","timestamp":1628279213707,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["class MultiInputForward(torch.nn.Module):\n","  def __init__(self, hidden_size, num_classes):\n","    super(MultiInputForward, self).__init__()\n","    self.name = \"rnn_mlp\"\n","    self.stationary = torch.nn.Sequential(\n","        torch.nn.Linear(13, 11),\n","        nn.ReLU(),\n","        torch.nn.Linear(11, 9),\n","        nn.ReLU(),\n","        torch.nn.Linear(9, 7),\n","        nn.ReLU()\n","    )\n","\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(27, hidden_size, num_layers = 5, batch_first=True)  # Change to RNN, LSTM, GRU for testing\n","    self.fc1 = nn.Linear(2*hidden_size, 32)  \n","    self.fc2 = nn.Linear(32, 16)\n","    self.fc3 = nn.Linear(23, 16) # Add last linear layer + 16\n","    self.out = nn.Linear(16, num_classes)\n","\n","  def forward(self, stationary_data, temporal_data):\n","    h0 = torch.zeros(5, len(temporal_data), self.hidden_size) #initial hidden state\n","    temporal, _ = self.rnn(temporal_data, h0) #forward propagation\n","    temporal = torch.cat([torch.max(temporal, dim=1)[0], torch.mean(temporal, dim=1)], dim=1)\n","    temporal = F.relu(self.fc1(temporal)) # pass output to classifier\n","    temporal = self.fc2(temporal)\n","\n","    stationary_result = self.stationary(stationary_data)\n","\n","    comb = torch.cat((stationary_result, temporal), 1)\n","    comb = F.relu(self.fc3(comb))\n","    comb = self.out(comb)\n","\n","    return comb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XakgSiiAFgd","executionInfo":{"status":"aborted","timestamp":1628279213707,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["class FinalModel(torch.nn.Module):\n","  def __init__(self, hidden_size, num_classes):\n","    super(FinalModel, self).__init__()\n","    self.name = \"final_model\"\n","    # ANN\n","    self.layer1 = nn.Linear(13, 10)\n","    self.layer2 = nn.Linear(10, 7)\n","    self.layer3 = nn.Linear(7, 5)\n","\n","    #RNN\n","    self.hidden_size = hidden_size\n","    self.rnn = nn.GRU(27, hidden_size, num_layers = 5, batch_first=True)  # Change to RNN, LSTM, GRU for testing\n","    #connect RNN to layers\n","    self.fc1 = nn.Linear(2*hidden_size, 48)  \n","    self.fc2 = nn.Linear(48, 32)\n","    self.fc3 = nn.Linear(32, 27)\n","\n","    #ANN+RNN -> ANN\n","    self.arlayer1 = nn.Linear(32, 16) # Add last linear layer + 27\n","    self.arlayer2 = nn.Linear(16, 8)\n","    self.out = nn.Linear(8, num_classes)\n","\n","  def forward(self, stationary_data, temporal_data):\n","    #ANN\n","    activation1 = F.relu(self.layer1(stationary_data))\n","    activation2 = F.relu(self.layer2(activation1))\n","    stationary_result = self.layer3(activation2)\n","\n","    #RNN\n","    h0 = torch.zeros(5, len(temporal_data), self.hidden_size) #initial hidden state\n","    temporal, _ = self.rnn(temporal_data, h0) #forward propagation\n","    temporal = torch.cat([torch.max(temporal, dim=1)[0], torch.mean(temporal, dim=1)], dim=1)\n","    temporal = F.relu(self.fc1(temporal)) # pass output to classifier\n","    temporal = F.relu(self.fc2(temporal))\n","    #temporal = F.relu(self.fc3(temporal))\n","    temporal = self.fc3(temporal)\n","\n","    #ANN+RNN -> ANN\n","    comb = torch.cat((stationary_result, temporal), 1)\n","    comb = F.relu(self.arlayer1(comb))\n","    comb = F.relu(self.arlayer2(comb))\n","    comb = self.out(comb)\n","\n","    return comb\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"am1j_RhvAFgd","executionInfo":{"status":"aborted","timestamp":1628279213707,"user_tz":240,"elapsed":12,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["def get_model_name(name, batch_size, learning_rate, epoch):\n","    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n","\n","    Args:\n","        config: Configuration object containing the hyperparameters\n","    Returns:\n","        path: A string with the hyperparameter name and value concatenated\n","    \"\"\"\n","    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n","                                                   batch_size,\n","                                                   learning_rate,\n","                                                   epoch)\n","    return path\n","\n","# Training Curve\n","def plot_training_curve(path):\n","    \"\"\" Plots the training curve for a model run, given the csv files\n","    containing the train/validation error/loss.\n","\n","    Args:\n","        path: The base path of the csv files produced during training\n","    \"\"\"\n","    import matplotlib.pyplot as plt\n","    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n","    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n","    train_acc = np.loadtxt(\"{}_train_acc.csv\".format(path))\n","    val_acc = np.loadtxt(\"{}_val_acc.csv\".format(path))\n","    plt.title(\"Train vs Validation Loss\")\n","    n = len(train_loss) # number of epochs\n","    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n","    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","    plt.title(\"Train vs Validation Accuracy\")\n","    plt.plot(range(1,n+1), train_acc, label=\"Train\")\n","    plt.plot(range(1,n+1), val_acc, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","def get_accuracy(model, loader):\n","    correct = 0\n","    total = 0\n","    for i, batches in enumerate(loader):\n","        for j, b in enumerate(batches):\n","            temporal_data, stationary_data, labels = b\n","            labels = labels.type(torch.LongTensor)\n","            output = model(stationary_data.float(), temporal_data.float())\n","            #select index with maximum prediction score\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(labels.view_as(pred)).sum().item()\n","            total += temporal_data.shape[0]\n","    return correct / total\n","\n","def get_loss(model, loader, criterion):\n","    total_loss = 0.0\n","    k = 0.0\n","    for i, batches in enumerate(loader):\n","        for j, b in enumerate(batches):\n","            k += 1.0\n","            temporal_data, stationary_data, label = b\n","            label = label.type(torch.LongTensor)\n","            pred = model(stationary_data.float(), temporal_data.float())\n","            loss = criterion(pred, label)\n","            total_loss += float(loss)\n","    return total_loss / k"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vV50QvjXAFgd","executionInfo":{"status":"aborted","timestamp":1628279213707,"user_tz":240,"elapsed":12,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","\n","\n","def train_model(model, train, val, num_epochs=5, learning_rate=1e-3, batch_size=16):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    train_losses, valid_losses, train_acc, val_acc = [], [], [], []\n","    epochs = []\n","    for epoch in range(num_epochs):\n","      train_loss = 0.0\n","      i = 0.0\n","      for i, batches in enumerate(train):\n","        for j, b in enumerate(batches):\n","            i += 1.0\n","            temporal_data, stationary_data, label = b\n","            label = label.type(torch.LongTensor)\n","            optimizer.zero_grad()\n","            pred = model(stationary_data.float(), temporal_data.float())\n","            loss = criterion(pred, label)\n","            # print(float(loss))\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += float(loss)\n","\n","      train_losses += [get_loss(model, train, criterion)]\n","      valid_losses += [get_loss(model, val, criterion)]\n","      epochs.append(epoch)\n","      train_acc.append(get_accuracy(model, train))\n","      val_acc.append(get_accuracy(model, val))\n","      print(\"Epoch %d; Training Loss %f; Validation Loss %f; Train Acc %f; Validation Acc %f\" % (\n","            epoch+1, train_losses[-1], valid_losses[-1], train_acc[-1], val_acc[-1]))\n","      if epoch > 30:\n","          # Save the current model (checkpoint) to a file\n","          model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n","          torch.save(model.state_dict(), model_path)\n","\n","    # plotting\n","    model_path = get_model_name(model.name, batch_size, learning_rate, epoch)\n","    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_losses)\n","    np.savetxt(\"{}_train_acc.csv\".format(model_path), train_acc)\n","    np.savetxt(\"{}_val_loss.csv\".format(model_path), valid_losses)\n","    np.savetxt(\"{}_val_acc.csv\".format(model_path), val_acc)\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(train_losses, label=\"Train\")\n","    plt.plot(valid_losses, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(epochs, train_acc, label=\"Train\")\n","    plt.plot(epochs, val_acc, label=\"Validation\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MRGFACdOAFge"},"source":["# TRAIN MODEL HERE"]},{"cell_type":"code","metadata":{"id":"Aw3HJnh_AFge","executionInfo":{"status":"aborted","timestamp":1628279213708,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["hidden_size = 27\n","num_classes = 5\n","batch_size = 32\n","\n","train_dataloaders = []\n","valid_dataloaders = []\n","test_dataloaders = []\n","\n","\n","for dataset in train_datasets:\n","    train_dataloaders += [DataLoader(dataset, shuffle=False, batch_size=batch_size)]\n","\n","for dataset in valid_datasets:\n","    valid_dataloaders += [DataLoader(dataset, shuffle=False, batch_size=batch_size)]\n","\n","for dataset in test_datasets:\n","    test_dataloaders += [DataLoader(dataset, shuffle=False, batch_size=batch_size)]\n","\n","train = MyLoader(train_dataloaders)\n","val = MyLoader(valid_dataloaders)\n","test = MyLoader(test_dataloaders)\n","\n","model = FinalModel(hidden_size, num_classes)\n","train_model(model, train, val, num_epochs =50, learning_rate=0.00725, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GTgwyBCQAFge","executionInfo":{"status":"aborted","timestamp":1628279213708,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["get_accuracy(model, test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5PUusj-WAFge"},"source":["# USE THIS CELL TO REPORT ANY SETS OF HYPERPARAMETERS THAT ARE GOOD (IF YOU WANT)\n"]},{"cell_type":"code","metadata":{"id":"wo6_m2EVAFge","executionInfo":{"status":"aborted","timestamp":1628279213709,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["##### Hyper Param Set # 1 ######\n","# num_epochs = \n","# learning_rate = \n","# batch_size = \n","# highest_accuracy = \n","\n","\n","##### Hyper Param Set # 2 ######\n","# num_epochs = \n","# learning_rate = \n","# batch_size = \n","# highest_accuracy = "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-ujJuqSAFge"},"source":["# ADDITIONAL ARCHITECHTURES TO TEST \n","- (IF POSSIBLE, SOPHIE COULD YOU MAKE YOUR OWN MODEL PLS, IT'S MAYBE NOT THE BEST IDEA TO COPY JOSHUA, WE SHOULD AT LEAST ADD MORE/REMOVE MORE LAYERS TO MAKE IT SOMEWHAT DIFFERENT)\n","- <3 <3 Don & Shahriyar"]},{"cell_type":"code","metadata":{"id":"aL9HJTvjAFge","executionInfo":{"status":"aborted","timestamp":1628279213709,"user_tz":240,"elapsed":13,"user":{"displayName":"Yi Qiu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00543704064574860365"}}},"source":["#this is the final model made by combining our architecture w joshua's code\n","#only need to fill in relevant blanks\n","\n","\n","class FinalModel(torch.nn.Module):\n","  def __init__(self, hidden_size, num_classes):\n","    super(FinalModel, self).__init__()\n","    self.name = \"final_model\"\n","    \n","    #ANN portion of the model\n","    self.layer1 = nn.Linear(13, 10)\n","    self.layer2 = nn.Linear(10, 8)\n","    self.layer3 = nn.Linear(8, 5)\n","\n","    #RNN portion of the model\n","    self.hidden_size = hidden_size\n","    '''for GRU\n","    self.rnn == nn.GRU(, hidden_size, num_layers = 5, batch_first=True)\n","    '''\n","#     for LSTM\n","    self.rnn = nn.LSTM(27, hidden_size, num_layers = 5, batch_first=True)\n","    self.fc = nn.Linear(hidden_size, num_classes)\n","\n","    #ANN+RNN fed into another ANN\n","    self.arlayer1 = nn.Linear(10, 9)\n","    self.arlayer2 = nn.Linear(9, 7)\n","    self.arlayer3 = nn.Linear(7,5)\n","\n","  def forward(self, x1, x2):\n","    #ANN portion of the model\n","    activation1 = F.relu(self.layer1(x1))\n","    activation2 = F.relu(self.layer2(activation1))\n","    x1_output = self.layer3(activation2)\n","\n","    #RNN portion of the model\n","    '''for GRU\n","    h0 = torch.zeros(5, x2.size(0), self.hidden_size)\n","    x2_output, _ = self.rnn(x2, h0)\n","    x2_output = torch.cat([torch.max(x2_output, dim=1)[0], torch.mean(x2_output, dim=1)], dim=1)\n","    x2_output = F.relu(self.fc(x2_output))\n","    '''\n","    #for LSTM\n","    h0 = torch.zeros(5, x2.size(0), self.hidden_size)\n","    c0 = torch.zeros(5, x2.size(0), self.hidden_size)\n","    x2_output, _ = self.rnn(x2, (h0, c0))\n","    x2_output = torch.cat([torch.max(x2_output, dim=1)[0], torch.mean(x2_output, dim=1)], dim=1)\n","    x2_output = F.relu(self.fc(x2_output))\n","    \n","\n","    #ANN+RNN fed into another ANN\n","    output = torch.cat((x1_output, x2_output), axis=1)\n","    output = F.relu(self.arlayer1(output))\n","    output = F.relu(self.arlayer2(output))\n","    output = self.arlayer3(output)\n","\n","    return output"],"execution_count":null,"outputs":[]}]}